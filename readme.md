# **Knowledge Distillation for Cancer Detection**

This project implements **Knowledge Distillation** to train a lightweight **Student Model** for cancer detection using a powerful **Teacher Model** that combines **MViT, ViT, YOLO, and CNN architectures**. The Student Model is optimized for **embedded systems** and **real-time inference** while maintaining high accuracy.

---

## ðŸš€ Project Overview

### ðŸ”¹ Teacher Model
The **Teacher Model** is an ensemble of powerful deep learning models:
- **MViT (Multi-scale Vision Transformer v2)** â€“ Extracts spatiotemporal features from video data.
- **ViT (Vision Transformer)** â€“ Captures global dependencies in image data.
- **YOLOv8** â€“ Performs fast object detection to enhance feature extraction.
- **ResNet-18 (CNN)** â€“ Provides strong feature representations.

All features are fused and passed through a classification head for **binary classification (Cancer/No Cancer).**

### ðŸ”¹ Student Model
A lightweight **CNN-based model** is trained using **Knowledge Distillation** to approximate the Teacher Model's knowledge while being optimized for **embedded devices**.

### ðŸ”¹ Knowledge Distillation
A **custom distillation loss** function combines:
1. **KL Divergence Loss** â€“ Encourages the student to mimic the teacher's soft outputs.
2. **Cross-Entropy Loss** â€“ Ensures correct classification.

---

## ðŸ“‚ Project Structure

```plaintext
ðŸ“¦ Knowledge-Distillation-Cancer-Detection
â”‚â”€â”€ ðŸ“‚ models
â”‚   â”œâ”€â”€ teacher_model.py  # Defines the Teacher Model
â”‚   â”œâ”€â”€ student_model.py  # Defines the Student Model
â”‚â”€â”€ ðŸ“‚ utils
â”‚   â”œâ”€â”€ distillation_loss.py  # Knowledge Distillation Loss function
â”‚   â”œâ”€â”€ dataset.py  # data loading
â”‚â”€â”€ ðŸ“‚ tests
â”‚   â”œâ”€â”€ test_models.py  # Unit tests for models
â”‚â”€â”€ main.py  # Entry point for training and evaluation
â”‚â”€â”€ requirements.txt  # Dependencies for the project
â”‚â”€â”€ README.md  # Project documentation
â”‚â”€â”€ test.py
â”‚â”€â”€ train.py
â”‚â”€â”€ config.py
